{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1: Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms # Python has packages for specific domains: torchvision, torchaudio, torchtext. These include datasets among others.\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data transformation and retrieval\n",
    "\n",
    "Here we will use the MNIST dataset, which we can load using a dedicated PyTorch module 'Dataset'.\n",
    "The Dataset module includes many built in datasets (full list: https://pytorch.org/vision/stable/datasets.html), but also supports custom datasets.\n",
    "Upon calling this module we will specify data transformations we want to apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a series of transformations to apply to the input images\n",
    "transformations = transforms.Compose([\n",
    "    # Transform a grayscale image to RGB. MNIST is originally in grayscale (1 channel with value range: 0-255)\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    # Resize the image to 256x256 pixels. This ensures that all input images have a uniform size.\n",
    "    transforms.Resize(256), \n",
    "    # Center crop the image to 224x224 pixels. This removes any borders and focuses on the central part of the image.\n",
    "    transforms.CenterCrop(224),\n",
    "    # Convert the image to a PyTorch tensor. This also effectively changes the value range from 0-255 to 0-1. \n",
    "    transforms.ToTensor(),\n",
    "    # Optionally, we will feed our dataset to a pretrained AlexNet (https://pytorch.org/hub/pytorch_vision_alexnet/). \n",
    "    # This model requires the images to be normalized using these values.\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# We then download the data using the specified transformations\n",
    "download_path = os.path.join(\"data\") # Specify location to store MNIST data. This is an OS-friendly way of specifying the path. Change as desired.\n",
    "# Create the folder if doesn't exist.\n",
    "if not os.path.exists(download_path):\n",
    "    os.makedirs(download_path)\n",
    "\n",
    "train_data = datasets.MNIST(download_path, train=True, download=True, transform=transformations) # Create a dataset for training\n",
    "test_data = datasets.MNIST(download_path, train=False, download=True, transform=transformations) # Create a dataset for testing (train=False!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch DataLoader\n",
    "PyTorch has another module called DataLoader (https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader).\n",
    "This module helps with sampling and iterating over the Dataset object created earlier. This also helps with sampling batches of images during training, instead of feeding single images.\n",
    "Additionally, it uses Python's multiprocessing functionality to sample data faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the batch size\n",
    "batch_size = 64\n",
    "\n",
    "# Create dataloaders for the training and test datasets respectively\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataLoader is a generator, meaning that it does not store its contents in memory (as with a Python list).\n",
    "In order to access its contents we need to loop over it, or alternatively call a batch with next(iter(DataLoader object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x000001DD942F60C0>\n"
     ]
    }
   ],
   "source": [
    "# Printing the dataloader object directly won't show its contents. Instead you see where the object itself is stored in memory.\n",
    "print(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           ...,\n",
       "           [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
       " \n",
       "          [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           ...,\n",
       "           [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
       " \n",
       "          [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           ...,\n",
       "           [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]],\n",
       " \n",
       " \n",
       "         [[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           ...,\n",
       "           [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
       " \n",
       "          [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           ...,\n",
       "           [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
       " \n",
       "          [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           ...,\n",
       "           [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]],\n",
       " \n",
       " \n",
       "         [[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           ...,\n",
       "           [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
       " \n",
       "          [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           ...,\n",
       "           [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
       " \n",
       "          [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           ...,\n",
       "           [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           ...,\n",
       "           [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
       " \n",
       "          [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           ...,\n",
       "           [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
       " \n",
       "          [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           ...,\n",
       "           [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]],\n",
       " \n",
       " \n",
       "         [[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           ...,\n",
       "           [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
       " \n",
       "          [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           ...,\n",
       "           [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
       " \n",
       "          [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           ...,\n",
       "           [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]],\n",
       " \n",
       " \n",
       "         [[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           ...,\n",
       "           [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
       " \n",
       "          [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           ...,\n",
       "           [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
       " \n",
       "          [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           ...,\n",
       "           [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]]),\n",
       " tensor([9, 1, 9, 1, 5, 2, 9, 4, 2, 1, 2, 0, 0, 0, 7, 1, 9, 0, 5, 5, 4, 5, 5, 4,\n",
       "         0, 4, 7, 7, 1, 1, 4, 2, 1, 7, 7, 0, 1, 2, 8, 4, 6, 1, 9, 9, 6, 8, 3, 7,\n",
       "         5, 5, 6, 5, 4, 9, 5, 6, 3, 1, 4, 0, 1, 8, 5, 3])]"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's have a look at what it yields.\n",
    "sample = next(iter(train_loader))\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dataloader returns both the image data, and the corresponding labels.\n",
    "Let's split these up and take a closer look at the structure of these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample_data shape: torch.Size([64, 3, 224, 224])\n",
      "Sample_labels shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "sample_images, sample_labels = next(iter(train_loader))\n",
    "print(f\"Sample_data shape: {sample_images.shape}\")\n",
    "print(f\"Sample_labels shape: {sample_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, both variables are batches of 50. The image additionally has 3 channels (RGB), each with a size of 224 by 224 pixels.\n",
    "Looking at 'sample_labels', we can see it contains the true digit labels of the corresponding images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 0, 7, 9, 9, 0, 7, 3, 5, 3, 6, 1, 6, 0, 0, 8, 7, 9, 2, 2, 6, 9, 9, 5,\n",
       "        6, 3, 9, 6, 4, 8, 2, 1, 9, 3, 8, 7, 2, 1, 0, 6, 2, 8, 9, 0, 7, 0, 9, 0,\n",
       "        6, 4, 0, 5, 4, 2, 1, 3, 2, 7, 0, 4, 2, 3, 9, 8])"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the first image as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_id = 0\n",
    "\n",
    "plt.imshow(sample_images[im_id].permute((1,2,0)))\n",
    "plt.title(f\"Image with label: {sample_labels[im_id]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 2: Create the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Device for Training\n",
    "Depending on your system, you can choose whether to train the model on your CPU (Central Processing Unit) or GPU (Graphical Processing Unit).\n",
    "Choosing to run the training process on your GPU will significantly increase the training speed. In PyTorch you can set the device to 'cuda' (Compute Unified Device Architecture), which is a sort of software interface that allows for general purpose computing on certain types of GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') # Configures to cuda if it is available.\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model Architecture\n",
    "Firstly, we need to define the model architecture. In other words, we need to define the amount and types of layers, in which order they are arranged, and how many inputs and outputs there are for each layer. Additionally, we define the activation functions that make the model non-linear. All this information is organized by means of a Python class. The model architecture will thus be contained within an object. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: We can either choose to highlight the different types of layers one-by-one here, or choose to explain the model line by line (less appropiate for self-study), or highlight the different layers after introducing the full model.\n",
    "\n",
    "We can put simple schematic pictures here to aid understanding\n",
    "\n",
    "- Flatten: transforms a 2d input to one dimension (see example below)\n",
    "- Linear: fully connected layer\n",
    "- ReLu: activation function to introduce non-linearity into the network, applies max(0,x)\n",
    "(nog meer uitleggen ook wat die init, en super is enzo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model structure\n",
    "class SimpleNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(224*224, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of how the Flatten function works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 150528])"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_image, sample_label = next(iter(test_loader))\n",
    "sample_image.to(device)\n",
    "flatten = nn.Flatten()\n",
    "output = flatten(sample_image)\n",
    "output.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To classify images (especially more naturalistic once) you might want to add convolutional layers to increase the performance of the model. \n",
    "Here you see the architecture of a simple model with one convolutional layer. \n",
    "\n",
    "If there is time left, we will come back to this and explain how it works. Otherwise, you can read through this explanation yourself if you are interested.\n",
    "\n",
    "...\n",
    "\n",
    "(maybe we can add a link here as well to the video of 3b1b explaining convolution?)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 3, out_channels = 10 , kernel_size = 3, stride = 1),\n",
    "            nn.BatchNorm2d(10),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 2)\n",
    "            )\n",
    "        \n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(10*110*110, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose which model you want to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = SimpleNeuralNetwork()\n",
    "model = ConvNeuralNetwork()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Prepare the training and testing of the model\n",
    "## Define function to train the model\n",
    "First we will define a function to train the model. This functions takes as input: \n",
    "- the dataloader: in this case that will be the dataloader with the training data\n",
    "- the model architecture\n",
    "- the loss function, which computes the dissimilarity between the network's predictions (pred) and the actual answer (y)\n",
    "- the optimizer functions, which computes the gradient based on the losses (dit moet anders verwoord zijn denk ik)\n",
    "- the current epoch to track the network's performance\n",
    "\n",
    "We set the model to training mode (model.train), this is important if you want to include batch normalization and dropout layers. Which we don't use here, but we include it for best practices.\n",
    "\n",
    "For each batch in the dataloader:\n",
    "1. we pass the input (X) through the model.\n",
    "2. The loss function calculates the dissimilarities between the prediction and the actual answer (y).\n",
    "3. the gradients of the losses are computed by taking the derivatives of the loss functions for each parameter (loss.backward).\n",
    "4. The optimizer than adjusts the parameters with these gradients (optimizer_step)\n",
    "5. the gradients are set back to zero to prevent the gradients to keep adding up across batches (optimizer.zero_grad).\n",
    "6. for each 10 and 100 batches we track the losses/progress of the training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, epoch):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #track the losses for visualization\n",
    "        if batch % 10 == 0:\n",
    "            train_losses.append(loss.item())\n",
    "            train_counter.append((batch*64) + (epoch*len(dataloader.dataset)))\n",
    "\n",
    "        #track progress of the training\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define function to test the model:\n",
    "Next we define a function to test the model. This function takes as input: \n",
    "- the dataloader: in this case that will be the dataloader with the test data\n",
    "- the model architecture\n",
    "- the loss function\n",
    "(we don't need the optimizer function, since we won't update the parameters anymore.)\n",
    "\n",
    "We set the model to evaluation mode (model.eval), this is important if you want to include batch normalization and dropout layers. Which we don't use here, but we include it for best practices.\n",
    "\n",
    "We initiate two empty variables to keep track of the model's performance\n",
    "\n",
    "We evaluate the model with torch.no_grad() to ensure that no gradients are computed during test mode (this also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True).\n",
    "\n",
    "We store the losses and correct answers to compute the accuracy and average loss of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    test_losses.append(test_loss)\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Train the model\n",
    "\n",
    "## Defining the hyperparameters\n",
    "Start by defining the hyperparameters:\n",
    "- epochs: the number of epochs = number of times that the training dataset is passed through the model\n",
    "- learning rate: value indicating to which extent the model parameters are updated for each batch/epoch (smaller values lead to slower updating).\n",
    "- loss function: PyTorch has different build in functions to calculate the losses\n",
    "- optimizer function: PyTorch has different build in functions for optimization\n",
    "\n",
    "Here we will use CrossEntropyLoss as the loss function. This function applies a softmax transformation to the models output to obtain the predicted class probabilities. Afterwards it computes the negative log-likelihood loss between the predicted probabilities and the true labels.\n",
    "\n",
    "For the optimizer we use stochastic gradient descent. (leggen we dat hier nog verder uit?)\n",
    "\n",
    "Next we will initiate a couple of empty lists to keep track of the network's performance\n",
    "\n",
    "## Test the untrained model\n",
    "First we test the untrained model, which performance do you expect?\n",
    "\n",
    "## Now everything is ready to train and test the network! \n",
    "We use a for loop to repeat the process for n epochs using the functions we created previously.\n",
    "After each training loop, we use the test loop to check the performance of the model.\n",
    "Note that the model thus also sees the test data for several times. \n",
    "To have a true evaluation of the model, you would actually need data that the model has never seen before (you could do this by splitting the data in the beginning when you are downloading it and creating the dataloaders).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define hyperparameters\n",
    "epochs = 1\n",
    "learning_rate = 1e-3\n",
    "loss_fn = nn.CrossEntropyLoss() #this step applies the softmax transformation and can compare the output of the model with integers\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(epochs + 1)]\n",
    "\n",
    "#test the untrained model\n",
    "test_loop(test_loader, model, loss_fn)\n",
    "\n",
    "#train the model\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_loader, model, loss_fn, optimizer, t)\n",
    "    test_loop(test_loader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the model's progress\n",
    "Now we can create a graph to see how the losses of the model go down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(train_counter, train_losses, color='cornflowerblue', linewidth=1.5)\n",
    "plt.scatter(test_counter, test_losses, zorder = 5, color='darkred')         \n",
    "plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
    "plt.xlabel('number of training examples seen')\n",
    "plt.ylabel('negative log likelihood loss')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model\n",
    "\n",
    "Lastly, you can store the network in different ways, depending on whether you only want to save its architecture or also the trained weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save  model\n",
    "torch.save(model, 'modelMNIST.pth')\n",
    "\n",
    "#if you would want to load the model again\n",
    "model = torch.load('modelMNIST.pth', weights_only=False), \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
